<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../assets/stylesheets/index.css">
    <title>about - sam webster</title>
</head>
<body>
    <div id="site_container">
        <div id="header">
            <div id="top-row" style="padding-bottom: 0; margin-bottom: 0;">
                <div style="margin: 0; padding: 0; width: 25%;">
                    <h1 id="tentwentythree" style="color: #22092C; margin-bottom: 0; padding-bottom: 0;">
                        <b class="link" style="margin: 0;" id="home-link">sam webster</b>
                    </h1>
                </div>
                <div style="margin: 0; padding: 0; display: flex; flex-direction: row; justify-content: space-between; width: 75%">
                    <h1 class="link" id="research-link">research</h1>
                    <h1 class="link" id="genart-link">generative art</h1>
                    <h1 class="link link-selected" id="about-link">about</h1>
                </div>
            </div>
            <div id="middle-row" style="padding-top: 0; margin-top: 0;">
                <h1 style="color: #22092C; margin-top: 0;">
                    phd student in computer science @ notre dame
                </h1>
            </div>
        </div>
    

        <div id="content-container" style="padding: 0; margin: 0; width: 100vw; display: flex; flex-direction: column; align-items: center;">
            <div id="brief" style="padding: 1em; text-align: center;">
                <h1>Links/Contacts:</h1>
                <h1 style="color: #22092C; margin-top: 0;">
                    <a href="../assets/other/swebster-cv-may25.pdf" target="_blank">Curriculum Vitae</a>
                </h1>
                <div id="socials-icons" style="display: flex; flex-direction: row; min-width: 100%; justify-content: space-between; align-items: center;">
                    <a href="https://www.linkedin.com/in/samjwebster/" target="_blank">
                        <img src="../assets/other/socials/linkedin.svg" alt="LinkedIn" style="width: 30px; height: 30px; margin: 0 1em; filter: brightness(0) saturate(100%) invert(7%) sepia(14%) saturate(7482%) hue-rotate(260deg) brightness(103%) contrast(104%);">
                    </a>
                    <a href="https://github.com/samjwebster/" target="_blank">
                        <img src="../assets/other/socials/github.svg" alt="GitHub" style="width: 30px; height: 30px; margin: 0 1em; filter: brightness(0) saturate(100%) invert(7%) sepia(14%) saturate(7482%) hue-rotate(260deg) brightness(103%) contrast(104%);">
                    </a>
                    <a href="https://scholar.google.com/citations?user=TJohAfIAAAAJ&hl=en" target="_blank">
                        <img src="../assets/other/socials/scholar.svg" alt="Google Scholar" style="width: 30px; height: 30px; margin: 0 1em; filter: brightness(0) saturate(100%) invert(7%) sepia(14%) saturate(7482%) hue-rotate(260deg) brightness(103%) contrast(104%);">
                    </a>
                </div>

            </div>


            <div id="content" style="padding: 1em; max-width: 75%;">
                <h1>* Education</h1>
                <div style="padding-left: 1em;">
                    <h2><b>University of Notre Dame</b></h2>
                    <div style="padding-left: 1em;">
                        <p><b>PhD</b> in Computer Science, May 2025 - present</p>
                        <p>Advisor: <a href="https://www.wjscheirer.com/" target="_blank">Prof. Walter J. Scheirer</a></p>

                        <p style="margin-top: 2em;"><b>MS</b> in Computer Science, Aug 2024 - May 2025</p>
                        <p>GPA: 4.0/4.0</p>
                        <p>Advisor: <a href="https://www.linkedin.com/in/adam-maciej-czajka/" target="_blank">Prof. Adam Czajka</a></p>

                        <p style="margin-top: 2em;"><b>BS</b> in Computer Science, Aug 2020 - May 2024</p>
                        <p>GPA: 3.5/4.0</p>
                    </div>
                </div>

                <h1 style="margin-top: 2em;">* Publications</h1>

                <div style="padding-left: 1em;">
                    <p>
                        “<a href="https://www.arxiv.org/abs/2505.02176" target="_blank">Saliency-Guided Training for Fingerprint Presentation Attack Detection</a>”<br>
                        2025 IEEE International Joint Conference on Biometrics (IJCB). Osaka, Japan, 2025.
                    </p>
                    <div style="padding-left: 1em;">
                        <h2><b>Samuel Webster</b> and Adam Czajka</h2>

                        <button type="button" class="collapsible">Abstract ▶</button>
                        <div class="collapsible-content" style="max-width: 600px; width: 80%;">
                            <p>Saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (PAD) tasks. This paper presents its first application to fingerprint PAD. We conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated "pseudosaliency," including minutiae-based, image quality-based, and autoencoder-based saliency maps. Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. Our findings demonstrate the effectiveness of saliency-guided training for fingerprint PAD in both limited and large data contexts, and we present a configuration capable of earning the first place on the LivDet-2021 benchmark. Our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint PAD. All collected saliency data and trained models are released with the paper to support reproducible research.</p>
                        </div>

                        <button type="button" class="collapsible" style="margin-top: 1em;">Presentations ▶</button>
                        <div class="collapsible-content" style="max-width: 600px; width: 80%;">
                            <p><b>Oral Presentations:</b></p>
                            <ul>
                                <li>
                                    “2025 IEEE International Joint Conference on Biometrics,” September 2025, Osaka, Japan.
                                </li>
                            </ul>

                            <p><b>Posters:</b></p>
                            <ul>
                                <li>
                                    “2025 IEEE International Joint Conference on Biometrics,” September 2025, Osaka, Japan.
                                </li>
                                <li>
                                    “University of Notre Dame Interdisciplinary Graduate Research Symposium,” April 2025, Notre Dame, IN.
                                </li>
                                <li>
                                    “3rd MSU-ND Computer Vision and Biometrics Workshop,” April 2025, Notre Dame, IN.
                                </li>
                            </ul>
                        </div>
                    </div>

                </div>

                <div style="padding-left: 1em;">
                    <p>
                        “<a href="https://livdet-iris.org/2025/" target="_blank">Iris Liveness Detection Competition (LivDet-Iris) - The 2025 Edition</a>”<br>
                        2025 IEEE International Joint Conference on Biometrics (IJCB). Osaka, Japan, 2025.
                    </p>
                    <div style="padding-left: 1em;">
                        <h2>Mahsa Mitcheff, Afzal Hossain, <b>Samuel Webster</b> et al.</h2>

                        <button type="button" class="collapsible">Abstract ▶</button>
                        <div class="collapsible-content" style="max-width: 600px; width: 80%;">
                            <p>LivDet-Iris 2025 is the sixth edition of the iris liveness detection competition. Held every two to three years, the competition aims to foster the development of robust algorithms capable of detecting a wide range of physically- and digitally-presented attacks in iris biometrics. The 2025 edition obtained the largest number of submissions in the history of the competition: ten algorithms from five institutions, and one commercial iris recognition system. LivDet-Iris 2025 also introduced new tasks compared to previous editions: (Task 1) a benchmark offered by an industry partner, (Task 2) morphed iris images, in which two different-identity samples were blended into one image, and (Task 3) evaluation of presentation attack detection robustness against advanced manufacturing techniques for textured contact lenses. This edition, for the first time in the series, offers a systematic testing of a commercial iris recognition system (software and hardware) using physical artifacts presented to the sensor. {\it Dermalog-Iris} team submitted algorithms that won all tasks, achieving the area under the ROC curve of 90.57\%, 68.23\% and 99.99\% in tasks 1, 2, and 3, respectively. Additionally, we include results for baseline algorithms, based on modern deep convolutional neural networks and trained with all available public datasets of iris images representing bona fide samples and anomalies (physical attacks, eye diseases, post-mortem cases, and synthetically-generated iris images). Test samples created for tasks 2 and 3, and baseline models are made available to offer the state-of-the-art benchmark for iris liveness detection.</p>
                        </div>

                        <button type="button" class="collapsible" style="margin-top: 1em;">Presentations ▶</button>
                        <div class="collapsible-content" style="max-width: 600px; width: 80%;">
                            <p><b>Oral Presentations:</b></p>
                            <ul>
                                <li>
                                    “2025 IEEE International Joint Conference on Biometrics,” September 2025, Osaka, Japan.
                                </li>
                            </ul>

                            <p><b>Posters:</b></p>
                            <ul>
                                <li>
                                    “2025 IEEE International Joint Conference on Biometrics,” September 2025, Osaka, Japan.
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div style="padding-left: 1em;">
                    <p style="margin-top: 2em;">
                        “<a href="https://arxiv.org/abs/2405.00650" target="_blank">Grains of Saliency: Optimizing Saliency-based Training of Biometric Attack Detection Models</a>”<br>
                        2024 IEEE International Joint Conference on Biometrics (IJCB). Buffalo, New York, United States, 2024.
                    </p>
                    <div style="padding-left: 1em;">
                        <h2>Colton R. Crum, <b>Samuel Webster</b>, Adam Czajka</h2>

                        <button type="button" class="collapsible">Abstract ▶</button>
                        <div class="collapsible-content" style="max-width: 600px; width: 80%;">
                            <p>Incorporating human-perceptual intelligence into model training has shown to increase the generalization capability of models in several difficult biometric tasks, such as presentation attack detection (PAD) and detection of synthetic samples. After the initial collection phase, human visual saliency (e.g., eye-tracking data, or handwritten annotations) can be integrated into model training through attention mechanisms, augmented training samples, or through human perception-related components of loss functions. Despite their successes, a vital, but seemingly neglected, aspect of any saliency-based training is the level of salience granularity (e.g., bounding boxes, single saliency maps, or saliency aggregated from multiple subjects) necessary to find a balance between reaping the full benefits of human saliency and the cost of its collection. In this paper, we explore several different levels of salience granularity and demonstrate that increased generalization capabilities of PAD and synthetic face detection can be achieved by using simple yet effective saliency post-processing techniques across several different CNNs.</p>
                        </div>

                        <button type="button" class="collapsible" style="margin-top: 1em;">Presentations ▶</button>
                        <div class="collapsible-content" style="max-width: 600px; width: 80%;">
                            <p><b>Oral Presentations:</b></p>
                            <ul>
                                <li>Spotlight at “2024 IEEE International Joint Conference on Biometrics,” September 2024, Buffalo, NY.</li>
                            </ul>
                            <p><b>Posters:</b></p>
                            <ul>
                                <li>“University of Notre Dame CSE Poster Competition and Recruitment Event,” March 2025, Notre Dame, IN.</li>
                                <li>“2024 IEEE International Joint Conference on Biometrics,” September 2024, Buffalo, NY.</li>
                                <li>“University of Notre Dame College of Science Joint Annual Meeting (COS-JAM)” May 2024, Notre Dame, IN.</li>
                            </ul>
                        </div>
                    </div>
                    
                </div>

                <h1 style="margin-top: 2em;">* Work Experience</h1>
                <div style="padding-left: 1em;">
                    <h2><b>University of Notre Dame</b></h2>
                    <div style="padding-left: 1em;">
                        <p><b>Research Assistant</b>, January 2024 - present</p>
                        <p>Research within the Notre Dame Computer Vision Research Lab.</p>
                        <p>See my publications above and my <a href="research.html">Research</a> page.</p>

                        <p style="margin-top: 2em;"><b>Teaching Assistant</b>, Aug 2023 - May 2024</p>
                        <p>Spring 2025 (Graduate TA): “Introduction to Artificial Intelligence”</p>
                        <p>Fall 2025 (Graduate TA): “Special Studies: Computer Vision”</p>
                        <p>Fall 2024 (Undergraduate TA): “Introduction to Artificial Intelligence”</p>
                    </div>

                    <h2 style="margin-top: 2em;"><b>Walmart Global Tech</b>, May - August 2023</h2>
                    <div style="padding-left: 1em;">
                        <p><b>Software Engineering II Intern</b>, Advertising Relevance Team</p>
                        <p>Developed Java Spring Boot backend for “Triad”, a diagnostic tool for ads on Walmart's website.</p>
                        <p>Optimized asynchronous database retrievals, improving retrieval speed by more than 500%.</p>
                    </div>

                    <h2 style="margin-top: 2em;"><b>Caktus AI</b>, January - May 2023</h2>
                    <div style="padding-left: 1em;">
                        <p><b>Machine Learning Data Analyst Intern</b></p>
                        <p>Developed a Django application for comparing LLM outputs to evaluate fit for Caktus's services.</p>
                    </div>


                   
                </div>
            </div>
        </div>

    </div>

    <script>
        // linking for header elements
        document.getElementById("home-link").addEventListener('click', function() {
            window.location.href = '../index.html';
        });
        document.getElementById("research-link").addEventListener('click', function() {
            window.location.href = './research.html';
        });
        document.getElementById("about-link").addEventListener('click', function() {
            window.location.href = './about.html';
        });
        document.getElementById("genart-link").addEventListener('click', function() {
            window.location.href = './genart.html';
        });
    </script>

    <script src="../assets/js/collapisble.js"></script>
    

</body>
</html>